{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e36988",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "#tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Conv2DTranspose, Flatten, MaxPooling2D, Add, LeakyReLU, \\\n",
    "                                    Reshape, Dropout, BatchNormalization, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from matplotlib import pyplot as plt\n",
    "initializer = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ebd49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image):\n",
    "    image = np.float32((image + 1) / 2)\n",
    "    plt.imshow(image[..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c691612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path, size=128, count=float('inf')):\n",
    "    paths = glob.glob(path+'/*.*')\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for n, i in enumerate(paths):\n",
    "        if n == count:\n",
    "            break\n",
    "        image = cv2.imread(i)\n",
    "        if len(image) >= size and len(image[0]) >= size:\n",
    "            images.append(cv2.resize(image, (size, size)))\n",
    "    images = np.uint8(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, path, size=128, batch_size=8, count=float('inf')):\n",
    "        self.images = load_images(path, size, count)\n",
    "        self.batch_size = batch_size\n",
    "    def get_random_samples(self):\n",
    "        indexes = np.random.randint(0, len(self.images), size=self.batch_size)\n",
    "        \n",
    "        ret = np.float16(self.images[indexes]) / 127.5 - 1\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = DataGenerator('./animefaces256cleaner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb90d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(datagen.get_random_samples()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    input_layer = Input(shape=(128,))\n",
    "    \n",
    "    ## PREPARE\n",
    "    dense_1 = Dense(512*8*8, kernel_initializer=initializer)(input_layer)\n",
    "    dense_1 = BatchNormalization()(dense_1)\n",
    "    dense_1 = ReLU()(dense_1)\n",
    "    \n",
    "    reshape = Reshape((8, 8, 512))(dense_1)\n",
    "    \n",
    "    conv_1 = Conv2DTranspose(256, 3, padding='same', strides=2, kernel_initializer=initializer)(reshape)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_1 = LeakyReLU(0.2)(conv_1)\n",
    "    \n",
    "    conv_1 = Conv2DTranspose(128, 3, padding='same', strides=2, kernel_initializer=initializer)(conv_1)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_1 = LeakyReLU(0.2)(conv_1)\n",
    "    \n",
    "    conv_1 = Conv2DTranspose(128, 3, padding='same', strides=1, kernel_initializer=initializer)(conv_1)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_1 = LeakyReLU(0.2)(conv_1)\n",
    "    \n",
    "    conv_1 = Conv2DTranspose(64, 3, padding='same', strides=2, kernel_initializer=initializer)(conv_1)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_1 = LeakyReLU(0.2)(conv_1)\n",
    "    \n",
    "    conv_1 = Conv2DTranspose(32, 3, padding='same', strides=2, kernel_initializer=initializer)(conv_1)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_1 = LeakyReLU(0.2)(conv_1)\n",
    "    \n",
    "    final = Conv2D(3, 9, padding='same', activation='tanh')(conv_1)\n",
    "\n",
    "    return Model(input_layer, final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52804ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    input_layer = Input(shape=(128, 128, 3), dtype=tf.float16)\n",
    "    \n",
    "    c1 = Conv2D(20, 4, padding='same', strides=1, kernel_initializer=initializer)(input_layer)\n",
    "    c1 = LeakyReLU(0.2)(c1)\n",
    "    \n",
    "    c2 = Conv2D(40, 4, padding='same', strides=2, kernel_initializer=initializer)(c1)\n",
    "    c2 = LeakyReLU(0.2)(c2)\n",
    "    \n",
    "    c2 = Conv2D(80, 4, padding='same', strides=2, kernel_initializer=initializer)(c2)\n",
    "    c2 = LeakyReLU(0.2)(c2)\n",
    "    \n",
    "    c2 = Conv2D(160, 4, padding='same', strides=2, kernel_initializer=initializer)(c2)\n",
    "    c2 = LeakyReLU(0.2)(c2)\n",
    "    \n",
    "    c2 = Conv2D(320, 4, padding='same', strides=2, kernel_initializer=initializer)(c2)\n",
    "    c2 = LeakyReLU(0.2)(c2)\n",
    "\n",
    "    flatten = Flatten()(c2)\n",
    "    \n",
    "    final = Dense(1)(flatten)\n",
    "\n",
    "    return Model(input_layer, final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc5ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = build_generator()\n",
    "disc = build_discriminator()\n",
    "\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c45ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(disc, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cbc422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = datagen.get_random_samples()[0]# + np.random.normal(size=(64, 64, 3), loc=0, scale=0.00)\n",
    "\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50164e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imshow(gen(np.random.normal(size=(1, 128)), training=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdfa9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(gen.predict(np.random.normal(size=(1, 128)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88651b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.predict(datagen.get_random_samples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(disc.predict(np.random.normal(size=(1, 128, 128, 3)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(prediction_fake):\n",
    "    ones = tf.ones_like(prediction_fake)\n",
    "    return loss(ones, prediction_fake)\n",
    "\n",
    "def discriminator_loss(prediction_fake, prediction_real):\n",
    "    ones = tf.random.uniform(minval=0.9, maxval=0.93, shape=prediction_real.shape)\n",
    "    #zeros = tf.random.uniform(minval=0, maxval=0.5, shape=prediction_fake.shape)\n",
    "    \n",
    "    #ones = tf.ones_like(prediction_real)\n",
    "    zeros = tf.zeros_like(prediction_fake)\n",
    "    \n",
    "    return loss(ones, prediction_real) + \\\n",
    "           loss(zeros, prediction_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7170ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    noise = tf.random.normal(shape=(BATCH_SIZE, 128))\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        fake_images = gen(noise, training=True)\n",
    "    \n",
    "        fake_desicions = disc(fake_images, training=True)\n",
    "        real_desicions = disc(real_images, training=True)\n",
    "    \n",
    "        gen_loss = generator_loss(fake_desicions)\n",
    "        disc_loss = discriminator_loss(fake_desicions, real_desicions)\n",
    "    \n",
    "    \n",
    "    generator_gradients = gen_tape.gradient(gen_loss, gen.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss, disc.trainable_variables)\n",
    "    \n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, disc.trainable_variables))\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, gen.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def generate_samle():\n",
    "    pred = gen(tf.random.normal(shape=(1, 128)))[0]\n",
    "    pred = (pred + 1) / 2 * 255\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c77e05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, 500_000):\n",
    "    batch = datagen.get_random_samples()\n",
    "    \n",
    "    try:\n",
    "        gl, dl = train_step(batch)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    gl = round(float(gl.numpy()), 4)\n",
    "    dl = round(float(dl.numpy()), 4)\n",
    "    print(f'\\rbatch: {i}\\tgl: {gl}\\tdl: {dl}\\t\\r')\n",
    "    \n",
    "    if (i % 200) == 0:\n",
    "        cv2.imwrite(f'samples/{i}.png', np.uint8(generate_samle()))\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    if (i + 1) % 300 == 0:\n",
    "        generator_optimizer.learning_rate = generator_optimizer.learning_rate * 0.995\n",
    "        discriminator_optimizer.learning_rate = discriminator_optimizer.learning_rate * 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c467ebb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator_optimizer.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26245d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.sum(np.float64(images), axis=0) / len(images)\n",
    "imshow(a) # mean of all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af372627",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.save('gen128_best2.h5')\n",
    "disc.save('disc128_best2.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
